{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ebgElQK6Zwch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto Tagging for Social Media Systems using NER\n",
        "\n",
        "\n",
        "**1.Data Collection:**\n",
        "\n",
        "The project uses a hardcoded dataset of sentences along with their corresponding entity tags. Each word in a sentence is tagged with an entity label (e.g., \"O\" for non-entities, \"B-LOC\" for the beginning of a location entity, and \"I-LOC\" for continuation).\n",
        "\n",
        "2**.Data Preparation:**\n",
        "\n",
        "Vocabulary and Tag Mapping:\n",
        "Create mappings for words to indices and tags to indices. This is essential for feeding the data into a machine learning model.\n",
        "Padding:\n",
        "Since sentences may vary in length, they are padded to a uniform length. This allows the model to process batches of data efficiently.\n",
        "\n",
        "**3.Model Building:**\n",
        "\n",
        "A Bidirectional Long Short-Term Memory (BiLSTM) model is constructed:\n",
        "Embedding Layer: Converts word indices into dense vectors of fixed size.\n",
        "BiLSTM Layer: Captures contextual information from both past and future words in the sentence, allowing for a better understanding of the entities.\n",
        "TimeDistributed Dense Layer: Outputs a probability distribution for each token in the sequence across all possible tags.\n",
        "\n",
        "**4.Training:**\n",
        "\n",
        "The model is trained on the prepared data using categorical cross-entropy as the loss function and Adam as the optimizer. The training process adjusts the model's parameters to minimize the loss and improve accuracy.\n",
        "\n",
        "**5.Evaluation:**\n",
        "\n",
        "After training, the model is evaluated on a separate test dataset to measure its performance in identifying entities.\n",
        "Prediction:\n",
        "\n",
        "The model can be used to predict entities in new sentences. Given a sentence, it converts words to their corresponding indices, pads the sequence, and generates predictions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dn2gS9nvZxMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "cbUkVOcLYnfg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z4S1lKYgW7R9"
      },
      "outputs": [],
      "source": [
        "# Sample sentences and corresponding tags\n",
        "train_sentences = [\n",
        "    ['I', 'love', 'New', 'York', 'City'],\n",
        "    ['San', 'Francisco', 'is', 'beautiful'],\n",
        "    ['Barack', 'Obama', 'was', 'the', 'president'],\n",
        "    ['Facebook', 'is', 'a', 'large', 'company'],\n",
        "    ['Apple', 'releases', 'new', 'iPhone', 'every', 'year'],\n",
        "    ['The', 'new', 'Google', 'Pixel', 'is', 'amazing'],\n",
        "    ['Tesla', 'is', 'innovating', 'in', 'electric', 'vehicles'],\n",
        "    ['I', 'met', 'Elon', 'Musk', 'yesterday'],\n",
        "    ['The', 'Golden', 'Gate', 'Bridge', 'is', 'in', 'San', 'Francisco'],\n",
        "    ['Microsoft', 'announced', 'Windows', '11'],\n",
        "    ['Amazon', 'is', 'headquartered', 'in', 'Seattle'],\n",
        "    ['Paris', 'is', 'known', 'for', 'the', 'Eiffel', 'Tower'],\n",
        "    ['The', 'CEO', 'of', 'Twitter', 'is', 'Elon', 'Musk'],\n",
        "    ['Bill', 'Gates', 'is', 'the', 'founder', 'of', 'Microsoft'],\n",
        "    ['New', 'York', 'is', 'often', 'called', 'the', 'Big', 'Apple'],\n",
        "    ['The', 'Great', 'Wall', 'of', 'China', 'is', 'ancient'],\n",
        "    ['I', 'enjoyed', 'the', 'movie', 'Inception'],\n",
        "    ['Leonardo', 'DiCaprio', 'starred', 'in', 'Inception'],\n",
        "    ['Amazon', 'Prime', 'has', 'great', 'shows'],\n",
        "    ['Google', 'announced', 'a', 'new', 'AI', 'feature']\n",
        "]\n",
        "\n",
        "train_tags = [\n",
        "    ['O', 'O', 'B-LOC', 'I-LOC', 'I-LOC'],\n",
        "    ['B-LOC', 'I-LOC', 'O', 'O'],\n",
        "    ['B-PER', 'I-PER', 'O', 'O', 'O'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'O'],\n",
        "    ['B-ORG', 'O', 'O', 'B-PROD', 'O', 'O'],\n",
        "    ['O', 'O', 'B-ORG', 'B-PROD', 'O', 'O'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'O', 'O'],\n",
        "    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n",
        "    ['O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'B-LOC', 'I-LOC'],\n",
        "    ['B-ORG', 'O', 'B-PROD', 'I-PROD'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'B-LOC'],\n",
        "    ['B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'],\n",
        "    ['O', 'O', 'O', 'B-ORG', 'O', 'B-PER', 'I-PER'],\n",
        "    ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-ORG'],\n",
        "    ['B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
        "    ['O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'O'],\n",
        "    ['O', 'O', 'O', 'O', 'B-PROD'],\n",
        "    ['B-PER', 'I-PER', 'O', 'O', 'B-PROD'],\n",
        "    ['B-ORG', 'B-PROD', 'O', 'O', 'O'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'B-TECH', 'O']\n",
        "]\n",
        "\n",
        "test_sentences = [\n",
        "    ['Google', 'is', 'based', 'in', 'Mountain', 'View'],\n",
        "    ['I', 'visited', 'the', 'Eiffel', 'Tower'],\n",
        "    ['Facebook', 'is', 'planning', 'a', 'new', 'feature'],\n",
        "    ['Tesla', 'unveils', 'new', 'Model', 'S'],\n",
        "    ['Apple', 'launched', 'the', 'new', 'iPad'],\n",
        "    ['Twitter', 'CEO', 'Elon', 'Musk', 'speaks', 'at', 'conference'],\n",
        "    ['Amazon', 'expands', 'to', 'new', 'markets'],\n",
        "    ['Microsoft', 'Teams', 'is', 'popular', 'in', 'offices'],\n",
        "    ['The', 'Great', 'Wall', 'of', 'China', 'is', 'a', 'tourist', 'attraction']\n",
        "]\n",
        "\n",
        "test_tags = [\n",
        "    ['B-ORG', 'O', 'O', 'O', 'B-LOC', 'I-LOC'],\n",
        "    ['O', 'O', 'O', 'B-LOC', 'I-LOC'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'O', 'B-TECH'],\n",
        "    ['B-ORG', 'O', 'O', 'B-PROD', 'I-PROD'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'B-PROD'],\n",
        "    ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O'],\n",
        "    ['B-ORG', 'O', 'O', 'O', 'B-LOC'],\n",
        "    ['B-ORG', 'B-PROD', 'O', 'O', 'O', 'O'],\n",
        "    ['O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary and tag sets\n",
        "vocab = list(set(word for sentence in train_sentences for word in sentence))\n",
        "tags = list(set(tag for tag_seq in train_tags for tag in tag_seq))"
      ],
      "metadata": {
        "id": "GOu_hZaiYEos"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {w: i + 2 for i, w in enumerate(vocab)}\n",
        "word2idx['UNK'] = 1  # Unknown words\n",
        "word2idx['PAD'] = 0  # Padding\n",
        "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2idx['PAD'] = 0  # Padding"
      ],
      "metadata": {
        "id": "3S3zOlTmYvq9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2tag = {i: w for w, i in tag2idx.items()}"
      ],
      "metadata": {
        "id": "47UQVVziY92R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [[word2idx.get(w, 1) for w in s] for s in train_sentences]\n",
        "y_train = [[tag2idx[t] for t in tag_seq] for tag_seq in train_tags]\n",
        "\n",
        "X_test = [[word2idx.get(w, 1) for w in s] for s in test_sentences]\n",
        "y_test = [[tag2idx[t] for t in tag_seq] for tag_seq in test_tags]"
      ],
      "metadata": {
        "id": "G08I0VziY_gg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences\n",
        "max_len = 10\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "y_train = pad_sequences(y_train, maxlen=max_len, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=max_len, padding='post')\n",
        "# Pad sequences\n",
        "max_len = 10\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "y_train = pad_sequences(y_train, maxlen=max_len, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=max_len, padding='post')\n"
      ],
      "metadata": {
        "id": "pxWLDHSkZE9l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to categorical\n",
        "y_train = [to_categorical(i, num_classes=len(tag2idx)) for i in y_train]\n",
        "y_test = [to_categorical(i, num_classes=len(tag2idx)) for i in y_test]"
      ],
      "metadata": {
        "id": "_TY-8yFVZJu7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "input = tf.keras.layers.Input(shape=(max_len,))\n",
        "model = tf.keras.layers.Embedding(input_dim=len(word2idx), output_dim=50, input_length=max_len)(input)\n",
        "model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(tag2idx), activation='softmax'))(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Biw1KrF3ZKrg",
        "outputId": "6f2585f1-5a75-4df3-98d7-116f9e11195d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Model(input, out)"
      ],
      "metadata": {
        "id": "VkeYT5fEZQNc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "BvD4Ad68ZU6W",
        "outputId": "ed854f79-4a6c-458b-8263-b082881fe915"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │           \u001b[38;5;34m3,950\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m200\u001b[0m)             │         \u001b[38;5;34m120,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)              │           \u001b[38;5;34m2,010\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,950</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">120,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,010</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,760\u001b[0m (495.16 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,760</span> (495.16 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m126,760\u001b[0m (495.16 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,760</span> (495.16 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=5, validation_split=0.1, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiNzcGPCZYLY",
        "outputId": "31e0b8de-cfe2-4dab-ab39-81b8b9830316"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.0389 - loss: 2.3029 - val_accuracy: 0.5000 - val_loss: 2.2812\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4889 - loss: 2.2826 - val_accuracy: 0.4500 - val_loss: 2.2599\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.4889 - loss: 2.2616 - val_accuracy: 0.4500 - val_loss: 2.2365\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.5056 - loss: 2.2382 - val_accuracy: 0.4500 - val_loss: 2.2099\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5333 - loss: 2.2117 - val_accuracy: 0.4500 - val_loss: 2.1789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "test_loss, test_acc = model.evaluate(X_test, np.array(y_test))\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9sZBxALZgLd",
        "outputId": "19e2f1b8-cda1-4cda-ffcb-794be1a9a2ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4667 - loss: 2.1866\n",
            "Test Loss: 2.1866328716278076\n",
            "Test Accuracy: 0.46666669845581055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prediction\n",
        "def predict(sentence):\n",
        "    sentence_idx = [word2idx.get(w, 1) for w in sentence]\n",
        "    sentence_padded = pad_sequences([sentence_idx], maxlen=max_len, padding='post')\n",
        "    pred = model.predict(sentence_padded)\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "    return [idx2tag[idx] for idx in pred[0] if idx != 0]\n",
        "\n",
        "example_sentence = ['I', 'love', 'San', 'Francisco']\n",
        "print(predict(example_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQHcalacZo9k",
        "outputId": "ef21c7e4-aba1-4eda-9002-531cd9745092"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step\n",
            "['O']\n"
          ]
        }
      ]
    }
  ]
}